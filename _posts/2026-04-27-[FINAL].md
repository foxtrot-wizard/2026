---
layout: distill
title: Sample Blog Post
description: Your blog post's abstract.
  Please add your abstract or summary here and not in the main body of your text.
  Do not include math/latex or hyperlinks.
date: 2026-04-27
future: true
htmlwidgets: true
hidden: true

# Mermaid diagrams
mermaid:
  enabled: true
  zoomable: true

authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2026-04-27-[FINAL].bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Understanding Non-Determinism in LLMs
    subsections:
      - name: The Probabilistic Foundation: Next Token Prediction
      - name: The Role of Sampling Strategies
      - name: Hardware-Level Non-Determinism
      - name: Batching Non-Determinism
      - name: Solutions and Mitigation (Deterministic Mode)
  - name: LLM Brittleness
    subsections:
      - name: The Probabilistic Foundation: Next Token Prediction
      - name: Formatting Brittleness
      - name: Positional Brittleness
      - name: How can be measured
      - name: How can be mitigated
      - name: Semantic vs Non-Semantic Brittleness
      - name: Can We Trust in the Brittleness?
  - name: LLM as evaluators
    subsections:
      - name: Interactive Figures
  - name: Conclusion



---

Large Language Models (LLMs) have rapidly graduated from experimental tools to the backbone of enterprise-level solutions <d-cite key="sora, chatgpt, cursor"></d-cite>. However as we move toward complex Agentic AI <d-cite key="react, agentrise"></d-cite>, where models must chain reasoning steps and interact with tools, we introduce multiple potential failure paths. In these high-stakes environments, finding the optimal prompt for a given task is often a tedious, onerous process requiring domain experts. Worse, even a carefully crafted prompt does not guarantee stability. LLMs often prove remarkably brittle <d-cite key="FormatSpread"></d-cite>, with predictions varying wildly based on negligible changes in phrasing.

To date, the community’s primary focus has been on studying a phenomenon called hallucination. It occurs when the models generate false or inconsistent content either because of gaps in the the knowledge presented in their training data, or simply fabrications, pure inventions [CITATION]. Recent literature [CITATION] argues that LLMs will always hallucinate, as it is not merely an occasional error, but an inherent feature of their probabilistic nature. Consequently, this has given rise to techniques focused on mitigating it, like In-Context Learning, Retrieval-Augmented Generation (RAG) and Instruction-Tuning.

In-Context Learning is [COMPLETE][CITATION]. Similarly, RAG has become a standard architecture for build robust LLM systems, by retrieving relevant, factual documents from an external knowledge base to the context window input, they guarantee a more trustworthy response [CITATION]. Finally, Instruction-Tuning is [COMPLETE][CITATION]

While these techniques address factual accuracy, they do not fully solve the "brittleness" of the models or their inherent non-determinism. A model whose output fluctuates based on trivial prompt variations is fundamentally unreliable for production systems. Furthermore, this instability casts doubt on methodological validation, particularly in popular evaluation techniques like "LLM-as-a-judge," where reproducibility is paramount. In this post, we aim to look beyond hallucination to discuss the challenges of reliability and reproducibility, and how we can overcome them.

---

## Prompt Brittleness

Prompt Brittleness refers to the phenomenon where minor and apparently marginal changes in format or structure are made to a LLM input prompt leads to an variation, sometimes drastically, in its performance <d-cite key="MoE"></d-cite>.  For software engineers integrating LLMs into their systems, brittleness creates a "dreadful challenge" in debugging. If a system works for one test case but fails for a nearly identical one due to a minor prompt variation, the system is effectively non-deterministic from the developer's perspective. 
Not even large and instruction-tuned models escapes from this sensitivity to “spurious” features <d-cite key="FormatSpread"></d-cite>. If a model fails because you used a different bracket style or change the input ordering, it begs the question where it is "understanding" the prompt in a human-like way or not.

Testing which format or ordering yields the best performance is not an easy task, as considering the full space of prompts formats makes the task a intractable problem, as computational cost increase linearly with the number of possible formats<d-cite key="FormatSpread"></d-cite>. Not only that, but it seems that prompt works in a non-monotonic logic,  where adding or removing text from it does not reflect in the final performance <d-cite key="FormatSpread"></d-cite>.

It seems that "brittleness" is not a single issue but it can manifests in lots of distinct dimensions of the input context. Here, three of them are highlighted:

### Wording Brittleness

One of the forms that “Brittle” manifest is from changes in specific phrasing of instructions, like prefixes and suffixes. Small semantic changes (e.g., "Let's think step by step" vs. "Let's work this out in a step by step way") can trigger significantly different reasoning paths and performance outcomes <d-cite key="APE"></d-cite>. 

This can be framed as  an optimization problem where human intuition is often insufficient to find these "magic words", and the engineering in charge of the prompt writing is trapped in a “prompt vibing” cage. This also has implication for the final end user, where a system might work perfectly for one user but fail for another who simply types differently, even if their intent is identical.

### Formatting Brittleness

Models are also sensitive to non-semantic modification. Spurious features like whitespace, capitalization, or the choice of brackets (e.g., [Input] vs. Input:) can cause accuracy to fluctuate by up to ****76% <d-cite key="FormatSpread"></d-cite>. This suggests that models rely heavily on surface-level patterns rather than deep semantic understanding of the task structure.

Studies <d-cite key="APE,MoE"></d-cite> also found that formats are not transferable by default, as a format having high performance for a model M, does not necessarily yields high performance for another model M*. This imply that a format is not inherently good or bad in a global sense, but a dependant of the model in question, and perhaps the training dataset. 

One of the most damaging fact draw from this conclusion is that semantically equivalent inputs do not yield consistent outputs, being a downside for reliability of the system created using LLMs.

### Positional Brittleness

Models are sensitive to where information is located. <d-cite key="LOSTINTHEMIDDLE"></d-cite> made an experiment based on problems of Multi option QA. Given a query and long list of documents, the goal was that the LLMS with large context windows had to find the right document somewhere in the middle of the list. The correct document was moved along the list to stress the capabilities of the model. Did the model always successful encounter the information its need? Not really! 

“Lost in the middle” phenomenon: Models are sensitive to where information is located. They exhibit a U-shaped performance curve, favoring information at the very beginning (primacy) or very end (recency) of the context while ignoring equally relevant information in the middle. This indicates a failure to treat the context window as a uniformly accessible memory space. Other works show how changing the options ordering degrades the systems performance on Multi option QA <d-cite key="orderbias"></d-cite>, and bias towards choosing a single option (e.g. often picking option D) <d-cite key="selectionbias"></d-cite>.

So retrieving more documents (e.g., in a RAG system) can harm performance if the relevant answer gets pushed to the middle of the context window. Thus system reliability often decreases as you provide *more* data, which is counter-intuitive for traditional software systems.

### How can be measured

A developer might deploy a model believing it is reliable, based on its accuracy from the testing data, only to find it behaving erratically in the wild because the production prompts differ slightly from the test prompt. This poses the question, “how can we quantify the sensitivity of an LLM to variations of the prompt?”

To quantify style-induced prompt brittleness, some works <d-cite key="MoE"></d-cite> deploys a simple metric, Spread: the difference between the performance of the best prompt (maximum accuracy) and the worst prompt (minimum accuracy), sample randomly from a test set. The premise is that  a system is only reliable if it performs well across a *distribution* of formats, not just a single "lucky" one. In a attempt to improve the quality of the sample space used to calculate the Spread, FormatSpread <d-cite key="FormatSpread"></d-cite> get ideas inspired from computer vision, where models learn from datasets with diverse styles. They use Thompson Sampling to efficiently sample formats, instead of a random approach. ****

While this method surely helps estimate a model sensitiveness using performance variance, Spread-like metrics includes the worst performing prompt found, and If this prompt was never choose in real life settings, it might only amplifying the perception of brittleness.

Despite of the fact that Spread is able to be used with other metrics besides accuracy, they are works that focuses in other types of measurements to quantify reliability. <d-cite key="QuantifyingSensitivity"></d-cite> presents two new metrics for classification tasks: **Sensitivity** and  **Consistency.** 

**Sensitivity**:  measures how much the model's predictions change when the prompt is rephrased. A key feature is that you don't need the "correct" answers (ground-truth labels) to calculate this, making it useful for real-world debugging.

**Consistency**: This metric measures how much the model's predictions vary for different items that all belong to the *same class* when the prompt is rephrased.

They prove that a model's **accuracy** and its **sensitivity,** how easily it is confused by rephrasing, are **not** strictly correlated, gaining new insights on how and where a failure might happen.  You cannot "optimize away" brittleness simply by chasing higher accuracy scores. A high-accuracy model might still fail catastrophically if the user phrases a request in a way the model didn't expect. A system with low consistency is unsafe for production because its behavior is unpredictable. The paper argues that **low sensitivity and high consistency** are actually more important than raw accuracy for building trustworthy systems.

**[EXAMPLE ON HOW THEY USE SENSITIVITY TO DEBUG]**

While calculating some of these metrics entails significant computational costs due to the volume of LLM calls required, they remain indispensable. Ensuring reliability in LLM-based systems is paramount, particularly for complex software integrations and high-stakes applications

### How can be mitigated

This objective comprises two distinct sub-problems: minimizing prompt sensitivity to brittleness (spurious features) and maximizing overall task performance. This objective comprises two distinct sub-problems: minimizing prompt sensitivity to brittleness (spurious features) and maximizing overall task performance.

<d-cite key="MoE"></d-cite> is a proposed ideia based in computer vision, where they present a diverse set of candidates, so the model can learn to disassociate the style from the target task. It does so by, during the inference, presenting the model to a mixture of different styles, for example, with few-shot examples in multiples styles during prompting.

<d-cite key="APE"></d-cite> focus in reduce human effort while the creation and validation of prompts. From a list of demonstration of input and output examples, a model is tasked to sample valid prompts that can generate these results. The highest-scoring instruction is selected after evaluation on a training subset. They demonstrate that LLMs can be deployed and are capable of self-generate prompts that are as good or better than human prompts in many tasks.

**[EXAMPLE FROM APE PAPER]**

Moreover, these studies shows that prompts are bound to the model they were generated for. They do not transfer well. If the underlying model changes (e.g., GPT-4 to Mistral), the optimized prompts from *APE* or the safe formats from Mixture of Formats ****might no longer be valid, requiring a complete re-optimization of the system to maintain reliability. Yet, it is desirable to have models that are robust to semantically equivalent variations of the initial prompt.

### Semantic vs Non-Semantic Brittleness

Human evaluators also exhibit "brittleness" when instructions are varied and some sensitivity is an inherent part of language understanding rather than a machine-specific bug. <d-cite key="HumanBrittle"></d-cite> shows that humans are also sensitive to prompt changes. When instructions change label sets or label formats, human annotations shift significantly.  This is particularly evident in subjective tasks, such as hate speech detection and emotion classification are inherently biased toward the value systems and personal experiences from the annotators <d-cite key="racialbias"></d-cite>. Distinct words carry different connotations, so a human could think differently about "Good" vs. "Positive." Furthermore, more extreme changes, such as adding an adverb to an adjective, result in greater shifts in performance.

If humans, the “gold” standard, vary their answers based on how a question is phrased, then zero variance in LLMs might actually be unnatural or indicative of rigid overfitting rather than true understanding.

So, Is it Brittleness a shared feature? The key distinction seems to lie in what kind changes are made to the prompt. While both humans and LLMs are sensitive to semantic changes, LLMs remain uniquely sensitive to these syntactic changes, like noise, typographical errors and label ordering <d-cite key="HumanBrittle"></d-cite>.

Defining 'brittleness' suffers due to a lack of standardization in the field.  The term can be viewed through a broad or strict lens, differing by separating "syntactic" brittleness from "natural" brittleness.

In summary, there is a shared semantic sensitivity for both humans and LLMs. However, the type of "brittleness" that comes from noise and small non-semantic changes might actually be a distinct feature of models. Humans understand that a typo doesn't change the task, whereas models often treat it as a completely different token distribution.

### Can We Trust in the Brittleness?

The fundamental opacity of Large Language Models creates a paradox where semantically identical prompts yield vastly different results, effectively turning prompt engineering into a "black-box optimization" problem. While we can empirically identify which inputs succeed, we lack the theory to explain why highly specific phrasing is required. If we cannot explain the strict requirements of the input, we cannot fully account for the generation of the output. Moreover, context position can also be viewed as a hidden variable in every explanation. A model might demonstrate good results in a short context, yet fail to perform in a longer one. This sensitivity raises serious questions regarding the methodological validity of current evaluations. It suggests that some "improvements" reported in literature may be artifacts of prompt engineering, essentially overfitting to a specific format, rather than genuine architectural gains. Furthermore, these underlying mechanics can be exploited to discover malicious jailbreaking prompts through high-dimensional search processes <d-cite key="JailbreakingHAY"></d-cite>.

To address this, reliability metrics may need to be re-weighted to distinguish between harmful instability, such as high sensitivity to typos, and natural ambiguity. Ultimately, as prompt engineering shifts toward automated, high-dimensional search, we must weigh the computational costs against the reality that we still do not understand why one prompt outperforms another.

---

## Conclusion

As LLMs might appear as inherently unreliable in ways that are difficult to predict or mitigate, we need to be specially aware of what they are capable, and what are the mainly problems when design new systems based on them. Achieving "enterprise-grade" reliability requires building complex wrapper systems (like automated prompt optimizers, RAGs, etc) to mitigate the model's inherent problems.

**[QUERIA UM PARAGRAFO SOBRE ISSO: EXPLICABILITY]**

 true explicability requires looking at these "low-level" mechanical factors

**[PARAGRAFO SOBRE AVALIAÇão]** 

We have to understanding things better and have more rigorous evaluation before making certain claims

**[SERIA LEGAL CITAR ISSO DE ALGUMA FORMA]**

works focus in more rigorous statitical evalution, 
targets the **"over-confidence"** in research findings that arises from lacking statistical rigor.

**[TALVEZ UM PARAGRAFO SOBRE ISSO?]** Reliance on Closed APIs: A significant reproducibility hurdle is the reliance on proprietary models

By adopting more rigorous statistical evaluations and maintaining a keen awareness of how brittleness affects our metrics, we can transition from deploying fragile models to engineering robust systems. While the underlying models may remain inherently probabilistic, disciplined engineering and targeted constraints in closed domains can render these errors statistically negligible, even more closed domain cases <d-cite key="negligible"></d-cite>, turning unpredictability into managed reliability.